{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Drago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s.,!?]', '', text)  # Keeps .,!? punctuation\n",
    "    text = text.lower()\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv('data/tweet_emotions.csv')\n",
    "df = df.drop(columns='tweet_id')\n",
    "\n",
    "df['content'] = df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 68055\n",
      "Max Length: 15\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(df['content']).split()\n",
    "vocab_size = len(set(all_words))\n",
    "print(f'Vocabulary Size: {vocab_size}')\n",
    "\n",
    "tweet_lengths = df['content'].apply(lambda x: len(x.split()))\n",
    "max_length = np.percentile(tweet_lengths, 95)\n",
    "max_length = int(max_length)\n",
    "print(f'Max Length: {max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content']\n",
    "Y = df['sentiment']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, truncating='post', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, truncating='post', padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Drago\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(13, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 61ms/step - accuracy: 0.2380 - loss: 2.1437 - val_accuracy: 0.3256 - val_loss: 1.9588\n",
      "Epoch 2/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.3758 - loss: 1.8362 - val_accuracy: 0.3293 - val_loss: 1.9728\n",
      "Epoch 3/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.4986 - loss: 1.4946 - val_accuracy: 0.3018 - val_loss: 2.2009\n",
      "Epoch 4/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60ms/step - accuracy: 0.6336 - loss: 1.1402 - val_accuracy: 0.2901 - val_loss: 2.5080\n",
      "Epoch 5/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.7282 - loss: 0.8644 - val_accuracy: 0.2784 - val_loss: 2.8079\n",
      "Epoch 6/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.7829 - loss: 0.6684 - val_accuracy: 0.2730 - val_loss: 3.3290\n",
      "Epoch 7/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.8222 - loss: 0.5439 - val_accuracy: 0.2707 - val_loss: 3.6945\n",
      "Epoch 8/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 58ms/step - accuracy: 0.8524 - loss: 0.4602 - val_accuracy: 0.2680 - val_loss: 4.3194\n",
      "Epoch 9/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.8743 - loss: 0.3882 - val_accuracy: 0.2537 - val_loss: 4.6447\n",
      "Epoch 10/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 58ms/step - accuracy: 0.8855 - loss: 0.3490 - val_accuracy: 0.2506 - val_loss: 5.2176\n",
      "Epoch 11/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60ms/step - accuracy: 0.8961 - loss: 0.3118 - val_accuracy: 0.2573 - val_loss: 5.6274\n",
      "Epoch 12/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60ms/step - accuracy: 0.9060 - loss: 0.2859 - val_accuracy: 0.2467 - val_loss: 5.8156\n",
      "Epoch 13/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.9134 - loss: 0.2602 - val_accuracy: 0.2442 - val_loss: 6.4314\n",
      "Epoch 14/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59ms/step - accuracy: 0.9211 - loss: 0.2391 - val_accuracy: 0.2359 - val_loss: 6.3304\n",
      "Epoch 15/15\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60ms/step - accuracy: 0.9260 - loss: 0.2234 - val_accuracy: 0.2414 - val_loss: 6.6578\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2414 - loss: 6.6349\n",
      "Test Loss: 6.657776355743408\n",
      "Test Accuracy: 0.24137499928474426\n"
     ]
    }
   ],
   "source": [
    "trained_model = model.fit(X_train_pad, Y_train, epochs=15, validation_data=(X_test_pad, Y_test), batch_size=32)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, Y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue know listenin bad habit earlier sta...</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed headache ughhhh...waitin call...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony...gloomy friday...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants hang friends soon!</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someone houston ticke...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>johnlloydtaylor</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>happy mothers day love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>happy mothers day mommies there, woman man lon...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>niariley wassup beautiful!!! follow me!! peep ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>mopedronin bullet train tokyo gf visiting japa...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment  \\\n",
       "0      1956967341       empty   \n",
       "1      1956967666     sadness   \n",
       "2      1956967696     sadness   \n",
       "3      1956967789  enthusiasm   \n",
       "4      1956968416     neutral   \n",
       "...           ...         ...   \n",
       "39995  1753918954     neutral   \n",
       "39996  1753919001        love   \n",
       "39997  1753919005        love   \n",
       "39998  1753919043   happiness   \n",
       "39999  1753919049        love   \n",
       "\n",
       "                                                 content predicted_sentiment  \n",
       "0      tiffanylue know listenin bad habit earlier sta...               empty  \n",
       "1           layin n bed headache ughhhh...waitin call...             sadness  \n",
       "2                    funeral ceremony...gloomy friday...             sadness  \n",
       "3                               wants hang friends soon!            surprise  \n",
       "4      dannycastillo want trade someone houston ticke...               worry  \n",
       "...                                                  ...                 ...  \n",
       "39995                                    johnlloydtaylor             neutral  \n",
       "39996                             happy mothers day love                love  \n",
       "39997  happy mothers day mommies there, woman man lon...                love  \n",
       "39998  niariley wassup beautiful!!! follow me!! peep ...           happiness  \n",
       "39999  mopedronin bullet train tokyo gf visiting japa...                love  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv('data/tweet_emotions.csv')\n",
    "new_df['content'] = new_df['content'].apply(clean_text)\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_df['content'])\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "predictions = model.predict(new_padded)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "new_df['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79935"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['sentiment'] == new_df['predicted_sentiment']].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
